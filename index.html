<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation</title>
    <link rel="icon" type="image/x-icon" href="public/logo.ico">
    <link rel="stylesheet" href="styles.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <!-- Hero Section -->
    <section class="hero">
        <div class="container">
            <div class="hero-content">


                <h1 class="title">Mind-the-Glitch</h1>
                <h2 class="subtitle">Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation</h2>
                
                <div class="authors">
                    <a href="https://abdo-eldesokey.github.io/" class="author">Abdelrahman Eldesokey</a>
                    <a href="https://cemse.kaust.edu.sa/profiles/aleksandar-cvejic" class="author">Aleksandar Cvejic</a>
                    <a href="https://www.bernardghanem.com/" class="author">Bernard Ghanem</a>
                    <a href="https://peterwonka.net/" class="author">Peter Wonka</a>
                </div>
                
                <div class="conference">
                    <span class="conference-badge">NeurIPS 2025 (Spotlight)</span>
                </div>
                
                <div class="action-buttons">
                    <a href="https://arxiv.org/abs/2509.21989" class="btn btn-primary">
                        <i class="fas fa-file-pdf"></i>
                        ArXiv
                    </a>
                    <a href="https://github.com/abdo-eldesokey/mind-the-glitch" class="btn btn-secondary">
                        <i class="fab fa-github"></i>
                        Code (Coming Soon)
                    </a>
                    <a href="#" class="btn btn-secondary">
                        <i class="fas fa-robot"></i>
                        HuggingFace
                    </a>
                </div>
                
                <div class="tldr">
                    <h3>TL;DR</h3>
                    <p>We introduce the first method to both <strong>quantify and spatially localize</strong> visual inconsistencies in subject-driven image generation by disentangling visual and semantic features from diffusion model backbones.</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Teaser Video Section -->
    <section class="teaser-section">
        <div class="container">
            <h2 class="section-title">Teaser Video</h2>
            <div class="video-container">
                <video controls autoplay muted loop poster="public/video-poster.jpg">
                    <source src="public/mtg_teaser_sound.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
        </div>
    </section>

    <!-- Abstract Section -->
    <section class="abstract-section">
        <div class="container">
            <h2 class="section-title">Abstract</h2>
            <div class="abstract-content">
                <p>We propose a novel approach for disentangling visual and semantic features from the backbones of pre-trained diffusion models, enabling <em>visual correspondence</em> in a manner analogous to the well-established <em>semantic correspondence</em>.</p>
                
                <p>While diffusion model backbones are known to encode semantically rich features, they must also contain visual features to support their image synthesis capabilities. However, isolating these visual features is challenging due to the absence of annotated datasets.</p>
                
                <p>To address this, we introduce an automated pipeline that constructs image pairs with annotated semantic and visual correspondences based on existing subject-driven image generation datasets, and design a contrastive architecture to separate the two feature types.</p>
                
                <p>Leveraging the disentangled representations, we propose a new metric, <em>Visual Semantic Matching (VSM)</em>, that quantifies visual inconsistencies in subject-driven image generation. Empirical results show that our approach outperforms global feature-based metrics such as CLIP, DINO, and vision--language models in quantifying visual inconsistencies while also enabling spatial localization of inconsistent regions.</p>
                
                <p>To our knowledge, this is the first method that supports both quantification and localization of inconsistencies in subject-driven generation, offering a valuable tool for advancing this task.</p>
            </div>
        </div>
    </section>

    <!-- Main Content Sections -->
    <section class="content-section" id="dataset">
        <div class="container">
            <h2 class="section-title">Dataset Generation Pipeline</h2>
            <div class="section-header">
                <div class="section-image">
                    <img src="public/dataset.jpg" alt="Dataset Generation Pipeline" />
                </div>
            </div>
            <div class="section-content">
                <div class="pipeline-overview">
                    <p class="section-intro">We developed an <strong>automated pipeline</strong> to create high-quality image pairs with controlled visual inconsistencies for training our correspondence model.</p>
                </div>
                
                <div class="pipeline-steps">
                    <div class="step-item">
                        <div class="step-icon">
                            <i class="fas fa-database"></i>
                        </div>
                        <div class="step-content">
                            <h4>Foundation Dataset</h4>
                            <p>Starts from <strong>consistent subject pairs</strong> sourced from the Subjects200k dataset</p>
                        </div>
                    </div>
                    
                    <div class="step-item">
                        <div class="step-icon">
                            <i class="fas fa-cut"></i>
                        </div>
                        <div class="step-content">
                            <h4>Subject Segmentation</h4>
                            <p>Uses <strong>Grounded-SAM</strong> to precisely segment subjects and <strong>CleanDIFT</strong> to compute semantic correspondences</p>
                        </div>
                    </div>
                    
                    <div class="step-item">
                        <div class="step-icon">
                            <i class="fas fa-crosshairs"></i>
                        </div>
                        <div class="step-content">
                            <h4>Correspondence Extraction</h4>
                            <p>Samples <strong>high-confidence correspondences</strong> and extracts localized regions via SAM for precise matching</p>
                        </div>
                    </div>
                    
                    <div class="step-item">
                        <div class="step-icon">
                            <i class="fas fa-filter"></i>
                        </div>
                        <div class="step-content">
                            <h4>Quality Filtering</h4>
                            <p>Detects and filters <strong>ambiguous matches</strong> using skewness of similarity distributions to ensure data quality</p>
                        </div>
                    </div>
                    
                    <div class="step-item">
                        <div class="step-icon">
                            <i class="fas fa-magic"></i>
                        </div>
                        <div class="step-content">
                            <h4>Controlled Inconsistencies</h4>
                            <p>Applies <strong>diffusion-based inpainting (SDXL)</strong> to introduce controlled visual inconsistencies while preserving semantic content</p>
                        </div>
                    </div>
                    
                    <div class="step-item">
                        <div class="step-icon">
                            <i class="fas fa-check-circle"></i>
                        </div>
                        <div class="step-content">
                            <h4>Final Validation</h4>
                            <p>Ensures quality via <strong>LPIPS filtering</strong> and size/aspect ratio constraints for consistent dataset standards</p>
                        </div>
                    </div>
                </div>
                
                <!-- <div class="dataset-stats">
                    <div class="stat-card">
                        <div class="stat-number">5,000</div>
                        <div class="stat-label">Training Pairs</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">500</div>
                        <div class="stat-label">Validation Pairs</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-icon">
                            <i class="fas fa-layer-group"></i>
                        </div>
                        <div class="stat-label">Paired Masks & Prompts</div>
                    </div>
                </div> -->
            </div>
        </div>
    </section>

    <section class="content-section" id="architecture">
        <div class="container">
            <h2 class="section-title">Architecture</h2>
            <div class="section-header">
                <div class="section-image">
                    <img src="public/method.jpg" alt="Architecture Diagram" />
                </div>
                <div class="section-content">
                    <div class="architecture-overview">
                        <p class="section-intro">Our architecture leverages a <strong>frozen diffusion backbone</strong> with dual-branch processing to disentangle semantic and visual features for precise inconsistency detection.</p>
                    </div>
                    
                    <div class="architecture-components">
                        <div class="component-item">
                            <div class="component-icon">
                                <i class="fas fa-snowflake"></i>
                            </div>
                            <div class="component-content">
                                <h4>Frozen Diffusion Backbone</h4>
                                <p>Built on <strong>Stable Diffusion 2.1</strong> with frozen weights to preserve pre-trained knowledge while extracting rich multi-layer decoder features</p>
                            </div>
                        </div>
                        
                        <div class="component-item">
                            <div class="component-icon">
                                <i class="fas fa-code-branch"></i>
                            </div>
                            <div class="component-content">
                                <h4>Dual-Branch Architecture</h4>
                                <p>Features are processed through <strong>two separate aggregation networks</strong> one for <b>semantic</b> and one for <b>visual</b> features</p>
                            </div>
                        </div>
                        
                        <!-- <div class="branch-details">
                            <div class="branch-card semantic-branch">
                                <div class="branch-header">
                                    <i class="fas fa-brain"></i>
                                    <h4>Semantic Branch (Ψs)</h4>
                                </div>
                                <p>Focuses on <strong>semantic consistency</strong> across image pairs, capturing high-level content understanding</p>
                            </div>
                            
                            <div class="branch-card visual-branch">
                                <div class="branch-header">
                                    <i class="fas fa-eye"></i>
                                    <h4>Visual Branch (Ψv)</h4>
                                </div>
                                <p>Targets <strong>fine-grained appearance</strong> details, detecting subtle visual inconsistencies</p>
                            </div>
                        </div> -->
                        
                        <div class="component-item">
                            <div class="component-icon">
                                <i class="fas fa-layer-group"></i>
                            </div>
                            <div class="component-content">
                                <h4>Feature Aggregation</h4>
                                <p>Each branch uses <strong>ResNet blocks per layer</strong> with trainable scalar weights for optimal feature combination</p>
                            </div>
                        </div>
                        
                        <div class="component-item">
                            <div class="component-icon">
                                <i class="fas fa-calculator"></i>
                            </div>
                            <div class="component-content">
                                <h4>Similarity Computation</h4>
                                <p>Computes <strong>semantic and visual similarity matrices</strong> via dot products for correspondence matching</p>
                            </div>
                        </div>
                        
                        <div class="component-item">
                            <div class="component-icon">
                                <i class="fas fa-balance-scale"></i>
                            </div>
                            <div class="component-content">
                                <h4>Contrastive Training</h4>
                                <p><strong>Semantic loss</strong> encourages alignment everywhere, while <strong>visual loss</strong> enforces similarity outside inconsistent regions and dissimilarity inside</p>
                            </div>
                        </div>
                    </div>
                    
                    <!-- <div class="architecture-benefits">
                        <div class="benefit-card">
                            <div class="benefit-icon">
                                <i class="fas fa-search"></i>
                            </div>
                            <div class="benefit-content">
                                <h4>Quantification</h4>
                                <p>Measures inconsistency levels across image pairs</p>
                            </div>
                        </div>
                        <div class="benefit-card">
                            <div class="benefit-icon">
                                <i class="fas fa-map-marker-alt"></i>
                            </div>
                            <div class="benefit-content">
                                <h4>Localization</h4>
                                <p>Pinpoints exact regions of visual inconsistencies</p>
                            </div>
                        </div>
                        <div class="benefit-card">
                            <div class="benefit-icon">
                                <i class="fas fa-puzzle-piece"></i>
                            </div>
                            <div class="benefit-content">
                                <h4>Disentanglement</h4>
                                <p>Separates semantic from visual features effectively</p>
                            </div>
                        </div>
                    </div> -->
                </div>
            </div>
        </div>
    </section>

    <section class="content-section" id="results">
        <div class="container">
            <div class="section-header">
                <h2 class="section-title">Results</h2>
                <div class="section-content">
                    <div class="results-overview">
                        <p class="section-intro">We visualize  <strong>the disentangled semantic and visual features</strong> to demonstrate the effectiveness of our approach.</p>
                        <div style="height: 2rem;"></div>
                    </div>
                </div>
                <div class="carousel-container">

                    <div class="carousel">
                        <div class="carousel-slide active">
                            <img src="public/testset/Slide1.JPG" alt="Results Slide 1" />
                        </div>
                        <div class="carousel-slide">
                            <img src="public/testset/Slide2.JPG" alt="Results Slide 2" />
                        </div>
                    </div>
                    <div class="carousel-controls">
                        <button class="carousel-btn prev-btn" onclick="changeSlide(-1)">
                            <i class="fas fa-chevron-left"></i>
                        </button>
                        <button class="carousel-btn next-btn" onclick="changeSlide(1)">
                            <i class="fas fa-chevron-right"></i>
                        </button>
                    </div>
                    <div class="carousel-indicators">
                        <span class="indicator active" onclick="currentSlide(1)"></span>
                        <span class="indicator" onclick="currentSlide(2)"></span>
                    </div>
                </div>
            
            </div>

        </div>
    </section>

    <!-- Evaluation Section -->
    <section class="content-section" id="evaluation">
        <div class="container">
            <h2 class="section-title">Evaluating Subject-Driven Image Generation Approaches</h2>
            <div class="section-header">
                <div class="evaluation-overview">
                    <p class="section-intro">To demonstrate the effectiveness of our approach on localizing and quantifying inconsistencies in real subject-driven image generation, we evaluate <strong>3 recent approaches</strong> and show that our proposed metric <strong>Visual Similarity Matching (VSM)</strong> based on feature correspondences is the most aligned with the groundtruth oracle compared to <strong>CLIP, DINO, and ChatGPT-4o</strong>.</p>
                    <div style="height: 2rem;"></div>
                </div>
                
                <div class="benchmark-carousel-container">
                    <div class="benchmark-carousel">
                        <div class="benchmark-slide active">
                            <img src="public/benchmark/Slide1.JPG" alt="Benchmark Results Slide 1" />
                        </div>
                        <div class="benchmark-slide">
                            <img src="public/benchmark/Slide2.JPG" alt="Benchmark Results Slide 2" />
                        </div>                        
                    </div>
                    <div class="benchmark-controls">
                        <button class="carousel-btn prev-btn" onclick="changeBenchmarkSlide(-1)">
                            <i class="fas fa-chevron-left"></i>
                        </button>
                        <button class="carousel-btn next-btn" onclick="changeBenchmarkSlide(1)">
                            <i class="fas fa-chevron-right"></i>
                        </button>
                    </div>
                    <div class="benchmark-indicators">
                        <span class="indicator active" onclick="currentBenchmarkSlide(1)"></span>
                        <span class="indicator" onclick="currentBenchmarkSlide(2)"></span>                        
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="content-section" id="reference">
        <div class="container">
            <div class="section-header">
                <h2 class="section-title"><i class="fas fa-quote-left"></i> Reference</h2>
            </div>
            <div class="section-content">
                <div class="citation-box">
                    <pre><code>@inproceedings{eldesokey2025mindtheglitch,
  title={Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation},
  author={Eldesokey, Abdelrahman and Cvejic, Aleksandar and Ghanem, Bernard and Wonka, Peter},
  booktitle={Advances in Neural Information Processing Systems},
  year={2025}
}</code></pre>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Mind-the-Glitch. All rights reserved.</p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>
